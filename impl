import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
from collections import deque
import random


class Actor(nn.Module):
  def __init__(self, state_dim, action_dim, max_action):
    super(Actor, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, action_dim),
        nn.Tanh()
    )
    self.max_action = max_action

  def forward(self, state):
    return self.max_action * self.net(state)


class Critic(nn.Module):
  def __init__(self, state_dim, action_dim):
    super(Critic, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim + action_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 1)
    )

  def forward(self, state, action):
    return self.net(torch.cat([state, action], dim=1))


class Model:
  def __init__(self, env: gym.Env, buffer_size=100000, batch_size=64, gamma=0.99, tau=0.005, actor_lr=1e-3, critic_lr=1e-3):
    """
    Initialize the Deep Deterministic Policy Gradient (DDPG) model.
    Args:
        env (gym.Env): The environment to train on.
        buffer_size (int): Maximum size of the replay buffer.
        batch_size (int): Number of samples per training batch.
        gamma (float): Discount factor.
        tau (float): Soft update factor for target networks.
        actor_lr (float): Learning rate for the actor network.
        critic_lr (float): Learning rate for the critic network.
    """
    self.env = env
    self.state_dim = env.observation_space.shape[0]
    self.action_dim = env.action_space.shape[0]
    self.max_action = env.action_space.high[0]

    # Actor and Critic networks
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    self.actor = Actor(self.state_dim, self.action_dim, self.max_action).to(device)
    self.actor_target = Actor(self.state_dim, self.action_dim, self.max_action).to(device)
    self.actor_target.load_state_dict(self.actor.state_dict())
    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)

    self.critic = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_target = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_target.load_state_dict(self.critic.state_dict())
    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)

    # Replay buffer
    self.buffer = deque(maxlen=buffer_size)
    self.batch_size = batch_size

    # Hyperparameters
    self.gamma = gamma
    self.tau = tau

  def store_transition(self, state, action, reward, next_state, done):
    self.buffer.append((state, action, reward, next_state, done))

  def sample_replay_buffer(self):
    batch = random.sample(self.buffer, min(len(self.buffer), self.batch_size))
    states, actions, rewards, next_states, dones = zip(*batch)
    return (
        torch.FloatTensor(states),
        torch.FloatTensor(actions),
        torch.FloatTensor(rewards).unsqueeze(1),
        torch.FloatTensor(next_states),
        torch.FloatTensor(dones).unsqueeze(1),
    )

  def update(self):
    if len(self.buffer) < self.batch_size:
      return

    states, actions, rewards, next_states, dones = self.sample_replay_buffer()

    # Update Critic
    with torch.no_grad():
      target_actions = self.actor_target(next_states)
      target_q = self.critic_target(next_states, target_actions)
      target_value = rewards + self.gamma * (1 - dones) * target_q

    current_q = self.critic(states, actions)
    critic_loss = nn.MSELoss()(current_q, target_value)
    self.critic_optimizer.zero_grad()
    critic_loss.backward()
    self.critic_optimizer.step()

    # Update Actor
    actor_loss = -self.critic(states, self.actor(states)).mean()
    self.actor_optimizer.zero_grad()
    actor_loss.backward()
    self.actor_optimizer.step()

    # Soft update target networks
    for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
    for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

  def train(self):
    """
    Train the model for one episode and return the episode reward.
    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      state_tensor = torch.FloatTensor(state).unsqueeze(0)
      action = self.actor(state_tensor).detach().numpy()[0]
      action = np.clip(action, -self.max_action, self.max_action)

      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      self.store_transition(state, action, reward, next_state, done)
      self.update()

      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward
      state = next_state

    return episode_reward, trajectory
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random


class ReplayBuffer:
  def __init__(self, buffer_size=100000, batch_size=256, large_batch_multiplier=4):
    self.buffer = deque(maxlen=buffer_size)
    self.batch_size = batch_size
    self.large_batch_size = batch_size * large_batch_multiplier

  def store(self, state, action, reward, next_state, done):
    self.buffer.append((state, action, reward, next_state, done))

  def sample_large_batch(self):
    return random.sample(self.buffer, min(len(self.buffer), self.large_batch_size))

  def sample_prioritized_batch(self, priorities):
    probabilities = priorities / np.sum(priorities)
    indices = np.random.choice(len(priorities), size=self.batch_size, p=probabilities)
    return indices

  def size(self):
    return len(self.buffer)

  def get_samples_by_indices(self, indices):
    return [self.buffer[i] for i in indices]


class Actor(nn.Module):
  def __init__(self, state_dim, action_dim, max_action):
    super(Actor, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, action_dim),
        nn.Tanh()
    )
    self.max_action = max_action

  def forward(self, state):
    return self.net(state) * self.max_action


class Critic(nn.Module):
  def __init__(self, state_dim, action_dim):
    super(Critic, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim + action_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 1)
    )

  def forward(self, state, action):
    return self.net(torch.cat([state, action], dim=1))


class Model:
  def __init__(self, env, buffer_size=100000, batch_size=256, gamma=0.99, tau=0.005, actor_lr=3e-4, critic_lr=3e-4, alpha=0.2):
    self.env = env
    self.state_dim = env.observation_space.shape[0]
    self.action_dim = env.action_space.shape[0]
    self.max_action = env.action_space.high[0]

    # Networks
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    self.actor = Actor(self.state_dim, self.action_dim, self.max_action).to(device)
    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)

    self.critic = Critic(self.state_dim, self.action_dim).to(device)
    self.target_critic = Critic(self.state_dim, self.action_dim).to(device)
    self.target_critic.load_state_dict(self.critic.state_dict())
    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)

    # Replay Buffer
    self.replay_buffer = ReplayBuffer(buffer_size, batch_size)

    # Hyperparameters
    self.gamma = gamma
    self.tau = tau
    self.alpha = alpha

  def store_transition(self, state, action, reward, next_state, done):
    self.replay_buffer.store(state, action, reward, next_state, done)

  def sample_large_batch(self):
    return self.replay_buffer.sample_large_batch()

  def train(self):
    """
    Train the model for one episode and return the episode reward and rewards per step.

    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      # Select an action
      state_tensor = torch.FloatTensor(state).unsqueeze(0)
      action = self.actor(state_tensor).detach().numpy()[0]
      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      # Store transition in the replay buffer
      self.store_transition(state, action, reward, next_state, done)

      # Perform training step
      self.train_step()

      # Update state and rewards
      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      state = next_state
      episode_reward += reward

    return episode_reward, trajectory

  def train_step(self):
    """
    Perform a single training step using a prioritized replay buffer.
    """
    if self.replay_buffer.size() < self.replay_buffer.batch_size:
      return

    # Sample large batch
    large_batch = self.sample_large_batch()
    states, actions, rewards, next_states, dones = zip(*large_batch)

    # Convert to tensors
    states = torch.FloatTensor(states)
    actions = torch.FloatTensor(actions)
    rewards = torch.FloatTensor(rewards).unsqueeze(1)
    next_states = torch.FloatTensor(next_states)
    dones = torch.FloatTensor(dones).unsqueeze(1)

    # Compute priorities
    with torch.no_grad():
      target_q_values = rewards + self.gamma * (1 - dones) * self.target_critic(next_states, self.actor(next_states))
    current_q_values = self.critic(states, actions)
    td_errors = torch.abs(current_q_values - target_q_values).detach().numpy()
    priorities = td_errors.flatten()

    # Down-sample to prioritized batch
    prioritized_indices = self.replay_buffer.sample_prioritized_batch(priorities)
    prioritized_samples = self.replay_buffer.get_samples_by_indices(prioritized_indices)
    states, actions, rewards, next_states, dones = zip(*prioritized_samples)

    states = torch.FloatTensor(states)
    actions = torch.FloatTensor(actions)
    rewards = torch.FloatTensor(rewards).unsqueeze(1)
    next_states = torch.FloatTensor(next_states)
    dones = torch.FloatTensor(dones).unsqueeze(1)

    # Critic update
    with torch.no_grad():
      target_q_values = rewards + self.gamma * (1 - dones) * self.target_critic(next_states, self.actor(next_states))
    current_q_values = self.critic(states, actions)
    critic_loss = nn.MSELoss()(current_q_values, target_q_values)

    self.critic_optimizer.zero_grad()
    critic_loss.backward()
    self.critic_optimizer.step()

    # Actor update
    actor_loss = -self.critic(states, self.actor(states)).mean()
    self.actor_optimizer.zero_grad()
    actor_loss.backward()
    self.actor_optimizer.step()

    # Soft update of target critic
    for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):
      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random


class ReplayBuffer:
  def __init__(self, buffer_size=100000, batch_size=256):
    self.buffer = deque(maxlen=buffer_size)
    self.batch_size = batch_size

  def store(self, state, action, reward, next_state, done):
    self.buffer.append((state, action, reward, next_state, done))

  def sample(self):
    batch = random.sample(self.buffer, min(len(self.buffer), self.batch_size))
    states, actions, rewards, next_states, dones = zip(*batch)
    return (
        torch.FloatTensor(states),
        torch.FloatTensor(actions),
        torch.FloatTensor(rewards).unsqueeze(1),
        torch.FloatTensor(next_states),
        torch.FloatTensor(dones).unsqueeze(1),
    )

  def size(self):
    return len(self.buffer)


class Actor(nn.Module):
  def __init__(self, state_dim, action_dim, max_action):
    super(Actor, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, action_dim),
        nn.Tanh()
    )
    self.max_action = max_action

  def forward(self, state):
    return self.net(state) * self.max_action


class Critic(nn.Module):
  def __init__(self, state_dim, action_dim):
    super(Critic, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim + action_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 1)
    )

  def forward(self, state, action):
    return self.net(torch.cat([state, action], dim=1))


class Model:
  def __init__(self, env, buffer_size=100000, batch_size=256, gamma=0.99, tau=0.005, alpha=0.2, actor_lr=3e-4, critic_lr=3e-4):
    self.env = env
    self.state_dim = env.observation_space.shape[0]
    self.action_dim = env.action_space.shape[0]
    self.max_action = env.action_space.high[0]

    # Networks
    self.actor = Actor(self.state_dim, self.action_dim, self.max_action).to(device)
    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)

    self.critic = Critic(self.state_dim, self.action_dim).to(device)
    self.target_critic = Critic(self.state_dim, self.action_dim).to(device)
    self.target_critic.load_state_dict(self.critic.state_dict())
    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)

    # Mean Actor
    self.mean_actor = Actor(self.state_dim, self.action_dim, self.max_action).to(device)
    self.mean_actor.load_state_dict(self.actor.state_dict())

    # Replay Buffer
    self.replay_buffer = ReplayBuffer(buffer_size, batch_size)

    # Hyperparameters
    self.gamma = gamma
    self.tau = tau
    self.alpha = alpha

  def store_transition(self, state, action, reward, next_state, done):
    self.replay_buffer.store(state, action, reward, next_state, done)

  def soft_update(self, target_net, source_net):
    for target_param, param in zip(target_net.parameters(), source_net.parameters()):
      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

  def train_step(self):
    if self.replay_buffer.size() < self.replay_buffer.batch_size:
      return

    states, actions, rewards, next_states, dones = self.replay_buffer.sample()

    # Critic update
    with torch.no_grad():
      next_actions = self.mean_actor(next_states)  # Use mean actor for stability
      target_q_values = rewards + self.gamma * (1 - dones) * self.target_critic(next_states, next_actions)

    current_q_values = self.critic(states, actions)
    critic_loss = nn.MSELoss()(current_q_values, target_q_values)

    self.critic_optimizer.zero_grad()
    critic_loss.backward()
    self.critic_optimizer.step()

    # Actor update with mean regularization
    actions = self.actor(states)
    actor_loss = -self.critic(states, actions).mean()

    # Mean regularization
    mean_actor_actions = self.mean_actor(states).detach()
    regularization = nn.MSELoss()(actions, mean_actor_actions)
    total_actor_loss = actor_loss + self.alpha * regularization

    self.actor_optimizer.zero_grad()
    total_actor_loss.backward()
    self.actor_optimizer.step()

    # Soft update of target networks and mean actor
    self.soft_update(self.target_critic, self.critic)
    self.soft_update(self.mean_actor, self.actor)

  def train(self):
    """
    Run an episode of training and return the episode reward and rewards per step.

    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      state_tensor = torch.FloatTensor(state).unsqueeze(0)
      action = self.actor(state_tensor).detach().numpy()[0]
      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      self.store_transition(state, action, reward, next_state, done)
      self.train_step()

      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward
      state = next_state

    return episode_reward, trajectory
import numpy as np
import gymnasium as gym
from stable_baselines3 import PPO


class Model:
  def __init__(self, env: gym.Env):
    """
    Initialize the PPO model.
    Args:
        env (gym.Env): The environment to train on.
    """
    self.env = env
    self.model = PPO("MlpPolicy", env, verbose=0)

  def train(self):
    """
    Train the model for one episode and return the episode reward.
    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    self.model.learn(total_timesteps=1000)  # Adjust timesteps as needed

    # Evaluate the model to get episode reward
    state, _ = self.env.reset()
    episode_reward = 0
    trajectory = []
    done = False

    while not done:
      action, _states = self.model.predict(state, deterministic=True)
      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward
      state = next_state

      episode_reward += reward

    return episode_reward, trajectory
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import gymnasium as gym
from collections import deque
import random


class Actor(nn.Module):
  def __init__(self, state_dim, action_dim, max_action):
    super(Actor, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
    )
    self.mean_layer = nn.Linear(256, action_dim)
    self.log_std_layer = nn.Linear(256, action_dim)
    self.max_action = max_action

  def forward(self, state):
    x = self.net(state)
    mean = self.mean_layer(x)
    log_std = self.log_std_layer(x).clamp(-20, 2)
    return mean, log_std

  def sample(self, state):
    mean, log_std = self(state)
    std = log_std.exp()
    normal = torch.distributions.Normal(mean, std)
    x_t = normal.rsample()  # Reparameterization trick
    action = torch.tanh(x_t) * self.max_action
    log_prob = normal.log_prob(x_t).sum(dim=-1)
    log_prob -= torch.log(1 - action.pow(2) + 1e-6).sum(dim=-1)
    return action, log_prob


class Critic(nn.Module):
  def __init__(self, state_dim, action_dim):
    super(Critic, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim + action_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 1)
    )

  def forward(self, state, action):
    return self.net(torch.cat([state, action], dim=1))


class Model:
  def __init__(self, env: gym.Env, buffer_size=1000000, batch_size=256, gamma=0.99, tau=0.005, alpha=0.2, actor_lr=3e-4, critic_lr=3e-4, target_entropy=None):
    """
    Initialize the Soft Actor-Critic (SAC) model.
    Args:
        env (gym.Env): The environment to train on.
        buffer_size (int): Maximum size of the replay buffer.
        batch_size (int): Number of samples per training batch.
        gamma (float): Discount factor.
        tau (float): Soft update factor for target networks.
        alpha (float): Temperature parameter for entropy regularization.
        actor_lr (float): Learning rate for the actor network.
        critic_lr (float): Learning rate for the critic networks.
        target_entropy (float): Target entropy for the policy.
    """
    self.env = env
    self.state_dim = env.observation_space.shape[0]
    self.action_dim = env.action_space.shape[0]
    self.max_action = env.action_space.high[0]

    # Actor
    self.actor = Actor(self.state_dim, self.action_dim, self.max_action).to(device)
    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)

    # Critics
    self.critic_1 = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_2 = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_target_1 = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_target_2 = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_target_1.load_state_dict(self.critic_1.state_dict())
    self.critic_target_2.load_state_dict(self.critic_2.state_dict())
    self.critic_optimizer_1 = optim.Adam(self.critic_1.parameters(), lr=critic_lr)
    self.critic_optimizer_2 = optim.Adam(self.critic_2.parameters(), lr=critic_lr)

    # Replay buffer
    self.buffer = deque(maxlen=buffer_size)
    self.batch_size = batch_size

    # Hyperparameters
    self.gamma = gamma
    self.tau = tau
    self.alpha = alpha
    self.target_entropy = target_entropy or -np.prod(env.action_space.shape).item()

  def store_transition(self, state, action, reward, next_state, done):
    self.buffer.append((state, action, reward, next_state, done))

  def sample_replay_buffer(self):
    batch = random.sample(self.buffer, min(len(self.buffer), self.batch_size))
    states, actions, rewards, next_states, dones = zip(*batch)
    return (
        torch.FloatTensor(states),
        torch.FloatTensor(actions),
        torch.FloatTensor(rewards).unsqueeze(1),
        torch.FloatTensor(next_states),
        torch.FloatTensor(dones).unsqueeze(1),
    )

  def update(self):
    if len(self.buffer) < self.batch_size:
      return

    states, actions, rewards, next_states, dones = self.sample_replay_buffer()

    # Update critics
    with torch.no_grad():
      next_actions, next_log_probs = self.actor.sample(next_states)
      target_q1 = self.critic_target_1(next_states, next_actions)
      target_q2 = self.critic_target_2(next_states, next_actions)
      target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
      target_value = rewards + self.gamma * (1 - dones) * target_q

    current_q1 = self.critic_1(states, actions)
    current_q2 = self.critic_2(states, actions)
    critic_loss_1 = nn.MSELoss()(current_q1, target_value)
    critic_loss_2 = nn.MSELoss()(current_q2, target_value)

    self.critic_optimizer_1.zero_grad()
    critic_loss_1.backward()
    self.critic_optimizer_1.step()

    self.critic_optimizer_2.zero_grad()
    critic_loss_2.backward()
    self.critic_optimizer_2.step()

    # Update actor
    actions, log_probs = self.actor.sample(states)
    actor_loss = (self.alpha * log_probs - self.critic_1(states, actions)).mean()

    self.actor_optimizer.zero_grad()
    actor_loss.backward()
    self.actor_optimizer.step()

    # Soft update target networks
    for target_param, param in zip(self.critic_target_1.parameters(), self.critic_1.parameters()):
      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
    for target_param, param in zip(self.critic_target_2.parameters(), self.critic_2.parameters()):
      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

  def train(self):
    """
    Train the model for one episode and return the episode reward.
    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      state_tensor = torch.FloatTensor(state).unsqueeze(0)
      action, _ = self.actor.sample(state_tensor)
      action = action.detach().numpy()[0]

      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      self.store_transition(state, action, reward, next_state, done)
      self.update()

      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward
      state = next_state

    return episode_reward, trajectory
import numpy as np
import gymnasium as gym


class Model:
  def __init__(self, env: gym.Env):
    """
    Initialize the SARSA model.
    Args:
        env (gym.Env): The environment to train on.
    """
    self.env = env
    self.q_table = np.zeros((self.env.observation_space.n, self.env.action_space.n))
    self.learning_rate = 0.1
    self.discount_factor = 0.99
    self.epsilon = 0.1  # For ε-greedy policy

  def epsilon_greedy_action(self, state):
    """
    Select an action using the ε-greedy policy.
    """
    if np.random.rand() < self.epsilon:
      return self.env.action_space.sample()
    else:
      return np.argmax(self.q_table[state])

  def train(self):
    """
    Train the model for one episode and return the episode reward.
    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    action = self.epsilon_greedy_action(state)
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      # Take the action and observe the next state and reward
      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      # Choose the next action using the ε-greedy policy
      next_action = self.epsilon_greedy_action(next_state)

      # Update Q-value using the SARSA formula
      td_target = reward + self.discount_factor * self.q_table[next_state, next_action]
      self.q_table[state, action] += self.learning_rate * (td_target - self.q_table[state, action])

      # Update state and action
      state, action = next_state, next_action

      # Accumulate rewards
      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward

    return episode_reward, trajectory
import numpy as np
import gymnasium as gym
from collections import deque
import random


class Model:
  def __init__(self, env: gym.Env, buffer_size=1000, batch_size=32):
    """
    Initialize the SARSA model with experience replay.
    Args:
        env (gym.Env): The environment to train on.
        buffer_size (int): Maximum size of the replay buffer.
        batch_size (int): Number of samples to train on from the replay buffer.
    """
    self.env = env
    self.q_table = np.zeros((self.env.observation_space.n, self.env.action_space.n))
    self.learning_rate = 0.1
    self.discount_factor = 0.99
    self.epsilon = 0.1  # For ε-greedy policy
    self.buffer = deque(maxlen=buffer_size)
    self.batch_size = batch_size

  def epsilon_greedy_action(self, state):
    """
    Select an action using the ε-greedy policy.
    """
    if np.random.rand() < self.epsilon:
      return self.env.action_space.sample()
    else:
      return np.argmax(self.q_table[state])

  def store_transition(self, transition):
    """
    Store a transition in the replay buffer.
    Args:
        transition (tuple): (state, action, reward, next_state, next_action, done)
    """
    self.buffer.append(transition)

  def sample_replay_buffer(self):
    """
    Sample a batch of transitions from the replay buffer.
    Returns:
        list: A batch of transitions.
    """
    return random.sample(self.buffer, min(len(self.buffer), self.batch_size))

  def replay(self):
    """
    Perform experience replay to update Q-values.
    """
    batch = self.sample_replay_buffer()
    for state, action, reward, next_state, next_action, done in batch:
      td_target = reward
      if not done:
        td_target += self.discount_factor * self.q_table[next_state, next_action]
      self.q_table[state, action] += self.learning_rate * (td_target - self.q_table[state, action])

  def train(self):
    """
    Train the model for one episode and return the episode reward.
    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    action = self.epsilon_greedy_action(state)
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      # Take the action and observe the next state and reward
      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      # Choose the next action using the ε-greedy policy
      next_action = self.epsilon_greedy_action(next_state)

      # Store the transition in the replay buffer
      self.store_transition((state, action, reward, next_state, next_action, done))

      # Update state and action
      state, action = next_state, next_action

      # Accumulate rewards
      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward

      # Perform replay to update Q-values
      self.replay()

    return episode_reward, trajectory
import numpy as np
import gymnasium as gym


class Model:
  def __init__(self, env: gym.Env):
    """
    Initialize the Temporal Difference (TD) model.
    Args:
        env (gym.Env): The environment to train on.
    """
    self.env = env
    self.q_table = np.zeros((self.env.observation_space.n, self.env.action_space.n))
    self.learning_rate = 0.1
    self.discount_factor = 0.99
    self.epsilon = 0.1  # For ε-greedy policy

  def epsilon_greedy_action(self, state):
    """
    Select an action using the ε-greedy policy.
    """
    if np.random.rand() < self.epsilon:
      return self.env.action_space.sample()
    else:
      return np.argmax(self.q_table[state])

  def train(self):
    """
    Train the model for one episode and return the episode reward.
    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      # Choose an action
      action = self.epsilon_greedy_action(state)

      # Take the action, observe the next state and reward
      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      # Update Q-value
      best_next_action = np.argmax(self.q_table[next_state])
      td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action]
      self.q_table[state, action] += self.learning_rate * (td_target - self.q_table[state, action])

      # Update state and accumulate reward
      state = next_state
      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward

    return episode_reward, trajectory
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
from collections import deque
import random


class Actor(nn.Module):
  def __init__(self, state_dim, action_dim, max_action):
    super(Actor, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, action_dim),
        nn.Tanh()
    )
    self.max_action = max_action

  def forward(self, state):
    return self.max_action * self.net(state)


class Critic(nn.Module):
  def __init__(self, state_dim, action_dim):
    super(Critic, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim + action_dim, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 1)
    )

  def forward(self, state, action):
    return self.net(torch.cat([state, action], dim=1))


class Model:
  def __init__(self, env: gym.Env, buffer_size=100000, batch_size=64, gamma=0.99, tau=0.005, actor_lr=1e-3, critic_lr=1e-3, policy_noise=0.2, noise_clip=0.5, policy_freq=2):
    """
    Initialize the Twin Delayed Deep Deterministic Policy Gradient (TD3) model.
    Args:
        env (gym.Env): The environment to train on.
        buffer_size (int): Maximum size of the replay buffer.
        batch_size (int): Number of samples per training batch.
        gamma (float): Discount factor.
        tau (float): Soft update factor for target networks.
        actor_lr (float): Learning rate for the actor network.
        critic_lr (float): Learning rate for the critic network.
        policy_noise (float): Stddev of noise added to target actions.
        noise_clip (float): Max magnitude of noise added to target actions.
        policy_freq (int): Frequency of policy updates relative to critic updates.
    """
    self.env = env
    self.state_dim = env.observation_space.shape[0]
    self.action_dim = env.action_space.shape[0]
    self.max_action = env.action_space.high[0]

    # Actor and Critic networks
    self.actor = Actor(self.state_dim, self.action_dim, self.max_action).to(device)
    self.actor_target = Actor(self.state_dim, self.action_dim, self.max_action).to(device)
    self.actor_target.load_state_dict(self.actor.state_dict())
    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)

    self.critic_1 = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_2 = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_target_1 = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_target_2 = Critic(self.state_dim, self.action_dim).to(device)
    self.critic_target_1.load_state_dict(self.critic_1.state_dict())
    self.critic_target_2.load_state_dict(self.critic_2.state_dict())
    self.critic_optimizer_1 = optim.Adam(self.critic_1.parameters(), lr=critic_lr)
    self.critic_optimizer_2 = optim.Adam(self.critic_2.parameters(), lr=critic_lr)

    # Replay buffer
    self.buffer = deque(maxlen=buffer_size)
    self.batch_size = batch_size

    # Hyperparameters
    self.gamma = gamma
    self.tau = tau
    self.policy_noise = policy_noise
    self.noise_clip = noise_clip
    self.policy_freq = policy_freq
    self.policy_update_counter = 0

  def store_transition(self, state, action, reward, next_state, done):
    self.buffer.append((state, action, reward, next_state, done))

  def sample_replay_buffer(self):
    batch = random.sample(self.buffer, min(len(self.buffer), self.batch_size))
    states, actions, rewards, next_states, dones = zip(*batch)
    return (
        torch.FloatTensor(states),
        torch.FloatTensor(actions),
        torch.FloatTensor(rewards).unsqueeze(1),
        torch.FloatTensor(next_states),
        torch.FloatTensor(dones).unsqueeze(1),
    )

  def update(self):
    if len(self.buffer) < self.batch_size:
      return

    states, actions, rewards, next_states, dones = self.sample_replay_buffer()

    # Add noise to target actions
    noise = torch.normal(0, self.policy_noise, size=actions.shape).clamp(-self.noise_clip, self.noise_clip)
    target_actions = (self.actor_target(next_states) + noise).clamp(-self.max_action, self.max_action)

    # Compute target Q-values
    with torch.no_grad():
      target_q1 = self.critic_target_1(next_states, target_actions)
      target_q2 = self.critic_target_2(next_states, target_actions)
      target_q = rewards + self.gamma * (1 - dones) * torch.min(target_q1, target_q2)

    # Update critics
    current_q1 = self.critic_1(states, actions)
    current_q2 = self.critic_2(states, actions)
    critic_loss_1 = nn.MSELoss()(current_q1, target_q)
    critic_loss_2 = nn.MSELoss()(current_q2, target_q)

    self.critic_optimizer_1.zero_grad()
    critic_loss_1.backward()
    self.critic_optimizer_1.step()

    self.critic_optimizer_2.zero_grad()
    critic_loss_2.backward()
    self.critic_optimizer_2.step()

    # Delayed policy updates
    if self.policy_update_counter % self.policy_freq == 0:
      # Update actor
      actor_loss = -self.critic_1(states, self.actor(states)).mean()
      self.actor_optimizer.zero_grad()
      actor_loss.backward()
      self.actor_optimizer.step()

      # Soft update target networks
      for target_param, param in zip(self.critic_target_1.parameters(), self.critic_1.parameters()):
        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
      for target_param, param in zip(self.critic_target_2.parameters(), self.critic_2.parameters()):
        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
      for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

    self.policy_update_counter += 1

  def train(self):
    """
    Train the model for one episode and return the episode reward.
    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      state_tensor = torch.FloatTensor(state).unsqueeze(0)
      action = self.actor(state_tensor).detach().numpy()[0]
      action = np.clip(action, -self.max_action, self.max_action)

      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      self.store_transition(state, action, reward, next_state, done)
      self.update()

      state = next_state
      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward

    return episode_reward, trajectory
import numpy as np
import gymnasium as gym


class Model:
  def __init__(self, env: gym.Env, lambda_=0.9):
    """
    Initialize the TD(λ) model.
    Args:
        env (gym.Env): The environment to train on.
        lambda_ (float): The eligibility trace decay parameter.
    """
    self.env = env
    self.q_table = np.zeros((self.env.observation_space.n, self.env.action_space.n))
    self.eligibility_traces = np.zeros_like(self.q_table)
    self.learning_rate = 0.1
    self.discount_factor = 0.99
    self.lambda_ = lambda_  # Decay parameter for eligibility traces
    self.epsilon = 0.1  # For ε-greedy policy

  def epsilon_greedy_action(self, state):
    """
    Select an action using the ε-greedy policy.
    """
    if np.random.rand() < self.epsilon:
      return self.env.action_space.sample()
    else:
      return np.argmax(self.q_table[state])

  def train(self):
    """
    Train the model for one episode and return the episode reward.
    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    done = False
    episode_reward = 0
    trajectory = []

    # Reset eligibility traces
    self.eligibility_traces.fill(0)

    while not done:
      # Choose an action
      action = self.epsilon_greedy_action(state)

      # Take the action, observe the next state and reward
      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      # Compute TD error
      best_next_action = np.argmax(self.q_table[next_state])
      td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action]
      td_error = td_target - self.q_table[state, action]

      # Update eligibility trace
      self.eligibility_traces[state, action] += 1

      # Update Q-values and eligibility traces for all state-action pairs
      self.q_table += self.learning_rate * td_error * self.eligibility_traces
      self.eligibility_traces *= self.discount_factor * self.lambda_

      # Update state and accumulate reward
      state = next_state
      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward

    return episode_reward, trajectory
import torch
from collections import deque


class Model:
  def __init__(self, env, alpha=0.01, gamma=0.99, lambd=0.9, buffer_size=10000):
    """
    True Online TD(lambda) with Replay.

    Args:
        env (gym.Env): The Gymnasium environment.
        alpha (float): Learning rate.
        gamma (float): Discount factor.
        lambd (float): Trace decay parameter.
        buffer_size (int): Maximum size of the replay buffer.
    """
    self.env = env
    self.state_dim = env.observation_space.shape[0]
    self.action_dim = env.action_space.shape[0]

    self.alpha = alpha
    self.gamma = gamma
    self.lambd = lambd
    self.buffer_size = buffer_size

    # Initialize weights and eligibility trace
    self.theta = torch.zeros(self.state_dim + self.action_dim, requires_grad=False)
    self.e_trace = torch.zeros(self.state_dim + self.action_dim, requires_grad=False)

    # Replay buffer
    self.replay_buffer = deque(maxlen=buffer_size)

  def store_transition(self, state, action, reward, next_state):
    """
    Store a transition in the replay buffer.
    """
    self.replay_buffer.append((state, action, reward, next_state))

  def compute_td_error(self, state, action, reward, next_state):
    """
    Compute the TD error.
    """
    sa_current = torch.cat((state, action))
    sa_next = torch.cat((next_state, torch.zeros(self.action_dim)))
    v_current = torch.dot(self.theta, sa_current)
    v_next = torch.dot(self.theta, sa_next)
    td_error = reward + self.gamma * v_next - v_current
    return td_error

  def update(self, state, action, reward, next_state):
    """
    Update weights using True Online TD(lambda).
    """
    sa_current = torch.cat((state, action))
    td_error = self.compute_td_error(state, action, reward, next_state)

    # Update eligibility trace
    self.e_trace = self.gamma * self.lambd * self.e_trace + sa_current

    # Update weights
    self.theta += self.alpha * td_error * self.e_trace

  def replay(self):
    """
    Perform updates using the replay buffer.
    """
    for state, action, reward, next_state in self.replay_buffer:
      self.update(state, action, reward, next_state)

  def train(self):
    """
    Train the model for one episode.

    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      # Select a random action
      action = torch.FloatTensor(self.env.action_space.sample())

      # Take a step in the environment
      next_state, reward, terminated, truncated, _ = self.env.step(action.numpy())
      done = terminated or truncated

      # Convert to tensors
      state_tensor = torch.FloatTensor(state)
      action_tensor = torch.FloatTensor(action)
      next_state_tensor = torch.FloatTensor(next_state)

      # Store the transition and update
      self.store_transition(state_tensor, action_tensor, reward, next_state_tensor)
      self.update(state_tensor, action_tensor, reward, next_state_tensor)

      # Perform replay
      self.replay()

      # Update state and reward
      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward
      state = next_state

    return episode_reward, trajectory

import gymnasium as gym
from sb3_contrib import TRPO


class Model:
  def __init__(self, env: gym.Env, gamma=0.99, gae_lambda=0.95, target_kl=0.01):
    """
    Initialize the TRPO model with the given environment and hyperparameters.
    Args:
        env (gym.Env): The environment to train on.
        gamma (float): Discount factor for rewards.
        gae_lambda (float): Lambda for Generalized Advantage Estimation (GAE).
        target_kl (float): Target Kullback-Leibler divergence for trust region update.
    """
    self.env = env
    self.model = TRPO(
        policy="MlpPolicy",
        env=env,
        verbose=0,
        gamma=gamma,
        gae_lambda=gae_lambda,
        target_kl=target_kl,  # Correctly using target_kl here
        policy_kwargs={
            "net_arch": [64, 64],  # Example network architecture
        },
    )

  def train(self):
    """
    Train the TRPO model for one episode and return the episode reward and rewards per step.

    Returns:
        episode_reward (float): Total reward obtained in the episode.
        trajectory (list): The trajectory of the agent..
    """
    state, _ = self.env.reset()
    done = False
    episode_reward = 0
    trajectory = []

    while not done:
      action, _ = self.model.predict(state, deterministic=False)
      next_state, reward, terminated, truncated, _ = self.env.step(action)
      done = terminated or truncated

      trajectory.append({
          "state": state.tolist(),
          "action": action.tolist(),
          "reward": reward,
          "next_state": next_state.tolist(),
          "done": done
      })
      episode_reward += reward
      state = next_state

    # Perform a learning step with a fixed number of timesteps
    self.model.learn(total_timesteps=1000, reset_num_timesteps=False)

    return episode_reward, trajectory
